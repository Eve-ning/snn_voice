{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# M5 Model\n",
    "\n",
    "This notebook **heavily** references the tutorial: *Speech Command Classification with torchaudio*.\n",
    "- [Tutorial](https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html)\n",
    "\n",
    "## Derivations\n",
    "We simplified the tutorial by:\n",
    "- Wrapping the PyTorch model in PyTorch-Lightning, abstracting away all low-level nitty-gritty loops\n",
    "- The M5 Model is simplified in code by wrapping common blocks to avoid redundant code: See `cnn_m5.py`\n",
    "- Using PyTorch-Lightning `Trainer`, we limit the number of batches, to make it run quicker\n",
    "\n",
    "## M5 Model Reference\n",
    "[Research Paper](https://arxiv.org/pdf/1610.00087.pdf)\n",
    "Dai, Wei, et al. \"Very deep convolutional neural networks for raw waveforms.\" 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017.\n",
    "\n",
    "## Results\n",
    "Through these settings below, we're able to yield 77% accuracy after 5 epochs, see the TensorBoard for history.\n",
    "The model has not reach states of overfit, instead, was cut short as it takes a long time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda Backend!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\johnc\\anaconda3\\envs\\snn_voice\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | CNNMel | 49.9 K\n",
      "---------------------------------\n",
      "49.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "49.9 K    Total params\n",
      "0.199     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68bd4f89049447d1826b5fe3ef5317a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnc\\anaconda3\\envs\\snn_voice\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\johnc\\anaconda3\\envs\\snn_voice\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "158be3e43b5d4784a8e4d9ebb6486c7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89dc0a9e28794a2bbb2bda7c6de41710"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17d2448097824cf78a6b56bd116b19df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnc\\anaconda3\\envs\\snn_voice\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torchaudio.transforms import Vad\n",
    "\n",
    "from src.model.cnn_mel import CNNMel\n",
    "from src.model.lit_wrapper import LitWrapper\n",
    "from src.speech_command_dataset import SpeechCommandDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} Backend!\")\n",
    "\n",
    "ds = SpeechCommandDataset(batch_size=128, num_workers=0)  #, dl_kwargs={'pin_memory': True})\n",
    "train_dl, val_dl, test_dl = ds.dls()\n",
    "\n",
    "model = LitWrapper(CNNMel(len(ds.classes), n_time=81, n_channel=80), ds.classes, lr=0.01)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"cnn_mel/\",\n",
    "    max_epochs=5,\n",
    "    limit_train_batches=192,\n",
    "    limit_val_batches=32,\n",
    "    limit_predict_batches=1,\n",
    "    # profiler='simple'\n",
    "    # fast_dev_run=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "# pred = trainer.predict(model, dataloaders=test_dl)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
